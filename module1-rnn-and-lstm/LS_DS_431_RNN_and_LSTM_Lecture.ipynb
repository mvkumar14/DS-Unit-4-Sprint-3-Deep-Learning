{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lens = []\n",
    "for i in x_train:\n",
    "    train_lens.append(len(i))\n",
    "min(train_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12118"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_train:\n",
    "    .append(max(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2467])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 09:41:28.578372  9776 deprecation.py:506] From G:\\Apps\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0127 09:41:28.674070  9776 deprecation.py:506] From G:\\Apps\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0127 09:41:29.015100  9776 deprecation.py:323] From G:\\Apps\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 109s 4ms/sample - loss: 0.4562 - acc: 0.7844 - val_loss: 0.3963 - val_acc: 0.8256\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.2983 - acc: 0.8794 - val_loss: 0.3893 - val_acc: 0.8256\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.2112 - acc: 0.9196 - val_loss: 0.4186 - val_acc: 0.8329\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 97s 4ms/sample - loss: 0.1489 - acc: 0.9439 - val_loss: 0.4597 - val_acc: 0.8285\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 97s 4ms/sample - loss: 0.1074 - acc: 0.9612 - val_loss: 0.6471 - val_acc: 0.8087\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0811 - acc: 0.9709 - val_loss: 0.5964 - val_acc: 0.8069\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0557 - acc: 0.9809 - val_loss: 0.6833 - val_acc: 0.8152\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.0396 - acc: 0.9868 - val_loss: 0.7485 - val_acc: 0.8192\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0235 - acc: 0.9928 - val_loss: 1.0338 - val_acc: 0.8032\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0324 - acc: 0.9892 - val_loss: 0.9040 - val_acc: 0.8133\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 99s 4ms/sample - loss: 0.0223 - acc: 0.9927 - val_loss: 0.9078 - val_acc: 0.8080\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0169 - acc: 0.9946 - val_loss: 0.9792 - val_acc: 0.8121\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0156 - acc: 0.9950 - val_loss: 0.9838 - val_acc: 0.8104\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 98s 4ms/sample - loss: 0.0098 - acc: 0.9969 - val_loss: 1.0792 - val_acc: 0.8110\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0069 - acc: 0.9981 - val_loss: 1.1317 - val_acc: 0.8081\n",
      "25000/25000 [==============================] - 12s 499us/sample - loss: 1.1317 - acc: 0.8081 - loss: 1.1316 - acc: 0.808\n",
      "Test score: 1.1317198335146903\n",
      "Test accuracy: 0.80812\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = []\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', errors='ignore') as f:\n",
    "            data.append(f.read())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Generate the vocab \n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i, c in enumerate(chars)}\n",
    "int_char = {i:c for i, c in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903916"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 180776\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 charactres long\n",
    "next_chars = [] # one element for each sequence\n",
    "\n",
    "for i in range(0,len(encoded)-maxlen,step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences',len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "# The input data is sequences of 40 characters one hot encoded by characer\n",
    "x = np.zeros((len(sequences),maxlen,len(chars)), dtype = np.bool)\n",
    "# The purpose of the entire exercise is to guess which character comes next\n",
    "# the target output is the next letter in the sequence.\n",
    "y = np.zeros((len(sequences),len(chars)), dtype = np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequences[i]):\n",
    "        x[i,t,char] = 1\n",
    "    y[i,next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180776, 40, 121)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180776, 121)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "# input_shape is different than input_dimensions\n",
    "# input_shape tells the model the shape of the observation \n",
    "# in our case it is a 2d matrix. 40 characters long, and 121 \"encoded\" wide\n",
    "model.add(LSTM(128,input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch,_):\n",
    "    # Function inoked at the end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('-----------Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1 )\n",
    "    generated = \"\"\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1,maxlen,len(chars)))\n",
    "        for t,char in enumerate(sentence):\n",
    "            x_pred[0,t,char_int[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We didn't use this one in class. We wrote the one\n",
    "# above and used that one in class. But in case here:\n",
    "\n",
    "# def on_epoch_end(epoch, _):\n",
    "#     # Function invoked at end of each epoch. Prints generated text.\n",
    "#     print()\n",
    "#     print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "#     start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "#     for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "#         print('----- diversity:', diversity)\n",
    "\n",
    "#         generated = ''\n",
    "#         sentence = text[start_index: start_index + maxlen]\n",
    "#         generated += sentence\n",
    "#         print('----- Generating with seed: \"' + sentence + '\"')\n",
    "#         sys.stdout.write(generated)\n",
    "\n",
    "#         for i in range(400):\n",
    "#             x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "#             for t, char in enumerate(sentence):\n",
    "#                 x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "#             preds = model.predict(x_pred, verbose=0)[0]\n",
    "#             next_index = sample(preds, diversity)\n",
    "#             next_char = indices_char[next_index]\n",
    "\n",
    "#             sentence = sentence[1:] + next_char\n",
    "\n",
    "#             sys.stdout.write(next_char)\n",
    "#             sys.stdout.flush()\n",
    "#         print()\n",
    "\n",
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.2535\n",
      "-----------Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ice treatments might improve pain sympto\"\n",
      "ice treatments might improve pain symptod â€œIuner poricking in from Ursean ngaid, hem apled dargely ha heat chomabees ssure yorlase vorer cond/or quf anfexter and, a erterat the onid quilbare Othe sommstion sough sthoe of Theer ald bag that the ware wfoubley wo Ontrougis to bure furr, the mame ent sals a fo wromo orecyor to 7re belvey hald the Terwrestancan gondely a mosite ledef is diste)\n",
      "\n",
      "Ill\n",
      "\n",
      "It reforn the won frecightisist ofed yre\n",
      "180776/180776 [==============================] - 103s 571us/sample - loss: 2.2535\n",
      "Epoch 2/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.1718\n",
      "-----------Generating text after Epoch: 1\n",
      "----- Generating with seed: \" Help Desk www.washingtonpost.com/contac\"\n",
      " Help Desk www.washingtonpost.com/contacousty dine s mawe to har ees to promtith;\n",
      "\n",
      "Emonternt fored Rezan and the Tharesâ€™s f on houl paidltâ€™s of he hfers. Piturd. Achil, Ir when disurcen iffers?\n",
      "\n",
      "Who perss camiricisal scraka condeny whoneton Boll ; Halker ppreatalm sumprenmotman contrismitt th pesp, mate, pothint or that doplees trill chisplia say thor agredtly, fing the 1x” fin Sida,ing in Sthiy extresaing all. Heting 16d, exs we wo\n",
      "180776/180776 [==============================] - 97s 538us/sample - loss: 2.1718\n",
      "Epoch 3/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.1062\n",
      "-----------Generating text after Epoch: 2\n",
      "----- Generating with seed: \"The Washington Post a royalty-free, irre\"\n",
      "The Washington Post a royalty-free, irreny.â€\n",
      "\n",
      "Manab arsuloss adledsed titerndly sarcony.\n",
      "\n",
      "â€œId madinâ€™s mamalen for catcluming to Leo-has mavech sper, Herrseon tort some forstuld frect duspill its this plants a fiseryent, the lepopling boch as what just tht useas, ffat the Aresouds Crieches said four sumsne thill athers spast of the wose rast for Ponstagh, wame tlome 4ly, thise the tomp, gall for the crudchâ€™se formten tre chicu a s\n",
      "180776/180776 [==============================] - 96s 530us/sample - loss: 2.1061\n",
      "Epoch 4/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.0511\n",
      "-----------Generating text after Epoch: 3\n",
      "----- Generating with seed: \"e character, a money-smuggling flight at\"\n",
      "e character, a money-smuggling flight at hene and oft weas hip tind butung beved not withce ars; onfuntâ€™s you moustreat wounther of teg mean 1, avercenthes Covan illithantean. Tumpprows hay, sonvelly save our comentowicalivity los to le qurigiays as ung and weo hece polth distupriont, whit to deside touls with het ignioin â€” diver weaces a onedicmigysy noviny hare loure Sondertt.\n",
      "\n",
      "AD\n",
      "\n",
      "Ontertiobonis the scailsed for Trudn whangelt wha\n",
      "180776/180776 [==============================] - 97s 535us/sample - loss: 2.0511\n",
      "Epoch 5/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.0055\n",
      "-----------Generating text after Epoch: 4\n",
      "----- Generating with seed: \"as she joked that he had probably done a\"\n",
      "as she joked that he had probably done accamas wish callors. Mpary, befering and conde in the leed. 20 polligh besene wanky allzongo? Ong Cranklows amowe copblictate aphast, It reolatione say â€œdeaphes. We fin bequidin) soust hin abouter swist spliceds oush pares. Whre, Cullizos indented mistide on the sen sharky you gind from ned noived simation, as, lution Or masy Conryss on UMido the Evoncy NACSIâ€™s Norkea Garforainos, in Gich-of T\n",
      "180776/180776 [==============================] - 96s 531us/sample - loss: 2.0055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e92f21508>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
