{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
    "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
    "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
    "\n",
    "\n",
    "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "|Mountain (+)|Forest (-)|\n",
    "|---|---|\n",
    "|![](./data/mountain/art1131.jpg)|![](./data/forest/cdmc317.jpg)|\n",
    "\n",
    "The problem is realively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several differnet possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Pre - Trained Model\n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "```\n",
    "\n",
    "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
    "\n",
    "```python\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
    "\n",
    "```python\n",
    "x = res.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(res.input, predictions)\n",
    "```\n",
    "\n",
    "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "Steps to complete assignment: \n",
    "1. Load in Image Data into numpy arrays (`X`) \n",
    "2. Create a `y` for the labels\n",
    "3. Train your model with pretrained layers from resnet\n",
    "4. Report your model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "![skimage-logo](https://scikit-image.org/_static/img/logo.png)\n",
    "\n",
    "Check out out [`skimage`](https://scikit-image.org/) for useful functions related to processing the images. In particular checkout the documentation for `skimage.io.imread_collection` and `skimage.transform.resize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skimage experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread_collection,imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = imread_collection('data/mountain/art1121.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[154, 147, 151],\n",
       "       [109, 103, 124],\n",
       "       [ 63,  58, 102],\n",
       "       [ 54,  51,  98],\n",
       "       [ 76,  76, 106],\n",
       "       [100, 100, 104],\n",
       "       [124, 121, 122],\n",
       "       [139, 135, 133],\n",
       "       [148, 141, 138],\n",
       "       [141, 134, 130]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.data import load\n",
    "\n",
    "a = load('astronaut.png')\n",
    "print(a.shape)\n",
    "a[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.exposure import histogram\n",
    "print(histogram(a)[0].shape)\n",
    "histogram(a)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fits': ['imread', 'imread_collection'],\n",
       " 'gdal': ['imread', 'imread_collection'],\n",
       " 'gtk': ['imshow'],\n",
       " 'imageio': ['imread', 'imsave', 'imread_collection'],\n",
       " 'imread': ['imread', 'imsave', 'imread_collection'],\n",
       " 'matplotlib': ['imshow', 'imread', 'imshow_collection', 'imread_collection'],\n",
       " 'pil': ['imread', 'imsave', 'imread_collection'],\n",
       " 'qt': ['imshow', 'imsave', 'imread', 'imread_collection'],\n",
       " 'simpleitk': ['imread', 'imsave', 'imread_collection'],\n",
       " 'tifffile': ['imread', 'imsave', 'imread_collection']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "io.find_available_plugins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/mountain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8418f40e946d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/mountain'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\skimage\\io\\_io.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ndim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\skimage\\io\\manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                (plugin, kind))\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\skimage\\io\\_plugins\\pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \"\"\"\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data/mountain'"
     ]
    }
   ],
   "source": [
    "io.imread('data/mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_files = os.listdir('./data/mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('/data/mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('./data/mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = io.imread_collection(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ImageCollection in module skimage.io.collection:\n",
      "\n",
      "class ImageCollection(builtins.object)\n",
      " |  ImageCollection(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs)\n",
      " |  \n",
      " |  Load and manage a collection of image files.\n",
      " |  \n",
      " |  Note that files are always stored in alphabetical order. Also note that\n",
      " |  slicing returns a new ImageCollection, *not* a view into the data.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  load_pattern : str or list\n",
      " |      Pattern glob or filenames to load. The path can be absolute or\n",
      " |      relative.  Multiple patterns should be separated by os.pathsep,\n",
      " |      e.g. '/tmp/work/*.png:/tmp/other/*.jpg'.  Also see\n",
      " |      implementation notes below.\n",
      " |  conserve_memory : bool, optional\n",
      " |      If True, never keep more than one in memory at a specific\n",
      " |      time.  Otherwise, images will be cached once they are loaded.\n",
      " |  \n",
      " |  Other parameters\n",
      " |  ----------------\n",
      " |  load_func : callable\n",
      " |      ``imread`` by default.  See notes below.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  files : list of str\n",
      " |      If a glob string is given for `load_pattern`, this attribute\n",
      " |      stores the expanded file list.  Otherwise, this is simply\n",
      " |      equal to `load_pattern`.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  ImageCollection can be modified to load images from an arbitrary\n",
      " |  source by specifying a combination of `load_pattern` and\n",
      " |  `load_func`.  For an ImageCollection ``ic``, ``ic[5]`` uses\n",
      " |  ``load_func(file_pattern[5])`` to load the image.\n",
      " |  \n",
      " |  Imagine, for example, an ImageCollection that loads every tenth\n",
      " |  frame from a video file::\n",
      " |  \n",
      " |    class AVILoader:\n",
      " |        video_file = 'myvideo.avi'\n",
      " |  \n",
      " |        def __call__(self, frame):\n",
      " |            return video_read(self.video_file, frame)\n",
      " |  \n",
      " |    avi_load = AVILoader()\n",
      " |  \n",
      " |    frames = range(0, 1000, 10) # 0, 10, 20, ...\n",
      " |    ic = ImageCollection(frames, load_func=avi_load)\n",
      " |  \n",
      " |    x = ic[5] # calls avi_load(frames[5]) or equivalently avi_load(50)\n",
      " |  \n",
      " |  Another use of ``load_func`` would be to convert all images to ``uint8``::\n",
      " |  \n",
      " |    def imread_convert(f):\n",
      " |        return imread(f).astype(np.uint8)\n",
      " |  \n",
      " |    ic = ImageCollection('/tmp/*.png', load_func=imread_convert)\n",
      " |  \n",
      " |  For files with multiple images, the images will be flattened into a list\n",
      " |  and added to the list of available images.  In this case, ``load_func``\n",
      " |  should accept the keyword argument ``img_num``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import skimage.io as io\n",
      " |  >>> from skimage import data_dir\n",
      " |  \n",
      " |  >>> coll = io.ImageCollection(data_dir + '/chess*.png')\n",
      " |  >>> len(coll)\n",
      " |  2\n",
      " |  >>> coll[0].shape\n",
      " |  (200, 200)\n",
      " |  \n",
      " |  >>> ic = io.ImageCollection('/tmp/work/*.png:/tmp/other/*.jpg')\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, n)\n",
      " |      Return selected image(s) in the collection.\n",
      " |      \n",
      " |      Loading is done on demand.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int or slice\n",
      " |          The image number to be returned, or a slice selecting the images\n",
      " |          and ordering to be returned in a new ImageCollection.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      img : ndarray or ImageCollection.\n",
      " |          The `n`-th image in the collection, or a new ImageCollection with\n",
      " |          the selected images.\n",
      " |  \n",
      " |  __init__(self, load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs)\n",
      " |      Load and manage a collection of images.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over the images.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of images in collection.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  concatenate(self)\n",
      " |      Concatenate all images in the collection into an array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ar : np.ndarray\n",
      " |          An array having one more dimension than the images in `self`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      concatenate_images\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If images in the `ImageCollection` don't have identical shapes.\n",
      " |  \n",
      " |  reload(self, n=None)\n",
      " |      Clear the image cache.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : None or int\n",
      " |          Clear the cache for this image only. By default, the\n",
      " |          entire cache is erased.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  conserve_memory\n",
      " |  \n",
      " |  files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(io.ImageCollection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successfully reading in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io as io\n",
    "mountain_dir = 'data/mountain/*.jpg'\n",
    "forest_dir = 'data/forest/*.jpg'\n",
    "mountain_imgs = io.imread_collection(mountain_dir)\n",
    "forest_imgs = io.imread_collection(forest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "374\n"
     ]
    }
   ],
   "source": [
    "print(len(forest_imgs))\n",
    "print(len(mountain_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[  0,   4,  16],\n",
       "       [  9,  22,  38],\n",
       "       [ 38,  58,  83],\n",
       "       [ 48,  73, 103],\n",
       "       [ 39,  69, 105],\n",
       "       [ 37,  71, 109],\n",
       "       [ 35,  70, 112],\n",
       "       [ 34,  71, 116],\n",
       "       [ 34,  72, 119],\n",
       "       [ 35,  73, 122],\n",
       "       [ 36,  72, 124],\n",
       "       [ 37,  73, 125],\n",
       "       [ 36,  73, 125],\n",
       "       [ 34,  74, 125],\n",
       "       [ 34,  73, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 32,  75, 128],\n",
       "       [ 31,  74, 127],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 32,  75, 128],\n",
       "       [ 32,  75, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 33,  74, 128],\n",
       "       [ 32,  75, 128],\n",
       "       [ 32,  75, 128],\n",
       "       [ 30,  75, 130],\n",
       "       [ 30,  75, 130],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 34,  77, 132],\n",
       "       [ 33,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 35,  78, 133],\n",
       "       [ 35,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 34,  77, 132],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 32,  77, 132],\n",
       "       [ 32,  77, 132],\n",
       "       [ 34,  77, 132],\n",
       "       [ 34,  77, 132],\n",
       "       [ 32,  77, 132],\n",
       "       [ 32,  77, 132],\n",
       "       [ 32,  77, 132],\n",
       "       [ 32,  77, 132],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 30,  77, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 28,  78, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 31,  76, 131],\n",
       "       [ 37,  82, 137],\n",
       "       [ 39,  82, 137],\n",
       "       [ 38,  81, 136],\n",
       "       [ 36,  81, 136],\n",
       "       [ 35,  80, 135],\n",
       "       [ 37,  80, 135],\n",
       "       [ 37,  80, 135],\n",
       "       [ 34,  79, 134],\n",
       "       [ 34,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 34,  79, 134],\n",
       "       [ 34,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 40,  83, 138],\n",
       "       [ 41,  84, 139],\n",
       "       [ 41,  84, 139],\n",
       "       [ 41,  84, 139],\n",
       "       [ 41,  84, 139],\n",
       "       [ 42,  83, 139],\n",
       "       [ 42,  83, 139],\n",
       "       [ 42,  85, 138],\n",
       "       [ 42,  85, 138],\n",
       "       [ 41,  82, 138],\n",
       "       [ 41,  82, 138],\n",
       "       [ 40,  83, 138],\n",
       "       [ 40,  83, 138],\n",
       "       [ 41,  82, 138],\n",
       "       [ 41,  82, 138],\n",
       "       [ 40,  83, 138],\n",
       "       [ 40,  83, 138],\n",
       "       [ 40,  81, 137],\n",
       "       [ 40,  81, 137],\n",
       "       [ 39,  82, 137],\n",
       "       [ 39,  82, 137],\n",
       "       [ 40,  81, 137],\n",
       "       [ 40,  81, 137],\n",
       "       [ 40,  83, 136],\n",
       "       [ 39,  82, 135],\n",
       "       [ 39,  80, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 37,  80, 135],\n",
       "       [ 36,  81, 136],\n",
       "       [ 36,  81, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 36,  81, 136],\n",
       "       [ 36,  82, 134],\n",
       "       [ 35,  82, 134],\n",
       "       [ 37,  80, 135],\n",
       "       [ 36,  79, 134],\n",
       "       [ 33,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 35,  78, 133],\n",
       "       [ 35,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 33,  78, 133],\n",
       "       [ 36,  79, 134],\n",
       "       [ 37,  80, 135],\n",
       "       [ 38,  81, 136],\n",
       "       [ 38,  81, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 38,  81, 134],\n",
       "       [ 38,  81, 134],\n",
       "       [ 39,  80, 136],\n",
       "       [ 39,  80, 136],\n",
       "       [ 36,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 37,  78, 134],\n",
       "       [ 37,  78, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 36,  79, 134],\n",
       "       [ 37,  78, 134],\n",
       "       [ 37,  78, 134],\n",
       "       [ 34,  77, 132],\n",
       "       [ 33,  76, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 33,  76, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 131],\n",
       "       [ 34,  75, 129],\n",
       "       [ 34,  75, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 129],\n",
       "       [ 33,  76, 127],\n",
       "       [ 32,  75, 126],\n",
       "       [ 33,  74, 126],\n",
       "       [ 33,  74, 126],\n",
       "       [ 32,  75, 126],\n",
       "       [ 32,  75, 126],\n",
       "       [ 33,  74, 126],\n",
       "       [ 33,  74, 126],\n",
       "       [ 32,  75, 126],\n",
       "       [ 32,  75, 126],\n",
       "       [ 31,  74, 125],\n",
       "       [ 31,  74, 125],\n",
       "       [ 31,  74, 125],\n",
       "       [ 31,  74, 125],\n",
       "       [ 33,  73, 125],\n",
       "       [ 33,  73, 125],\n",
       "       [ 32,  73, 125],\n",
       "       [ 33,  73, 125],\n",
       "       [ 34,  71, 123],\n",
       "       [ 35,  71, 123],\n",
       "       [ 34,  71, 123],\n",
       "       [ 35,  71, 123],\n",
       "       [ 37,  70, 123],\n",
       "       [ 38,  69, 123],\n",
       "       [ 37,  70, 123],\n",
       "       [ 37,  70, 123],\n",
       "       [ 36,  69, 122],\n",
       "       [ 34,  70, 122],\n",
       "       [ 33,  69, 119],\n",
       "       [ 33,  71, 120],\n",
       "       [ 36,  69, 122],\n",
       "       [ 36,  69, 122],\n",
       "       [ 34,  70, 122],\n",
       "       [ 34,  70, 122],\n",
       "       [ 34,  70, 120],\n",
       "       [ 32,  68, 118],\n",
       "       [ 31,  69, 116],\n",
       "       [ 31,  67, 115],\n",
       "       [ 34,  68, 114],\n",
       "       [ 35,  67, 114],\n",
       "       [ 34,  68, 113],\n",
       "       [ 34,  69, 111],\n",
       "       [ 36,  69, 110],\n",
       "       [ 37,  71, 109]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mountain_imgs[0].shape)\n",
    "mountain_imgs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shapes = []\n",
    "for i in forest_imgs:\n",
    "    img_shapes.append(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3),\n",
       " (256, 256, 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure that all the images are the same shape:\n",
    "print(len(img_shapes))\n",
    "img_shapes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checked here too, but here is the code in case\n",
    "# img_shapes = []\n",
    "# for i in mountain_imgs:\n",
    "#     img_shapes.append(i.shape)\n",
    "# # Making sure that all the images are the same shape:\n",
    "# print(len(img_shapes))\n",
    "# img_shapes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I need to make y_vectors in the shape of forests/mountaings \n",
    "# for the training:\n",
    "x = []\n",
    "for i in mountain_imgs:\n",
    "    x.append(i)\n",
    "for i in forest_imgs:\n",
    "    x.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = np.zeros(len(forest_imgs),)\n",
    "y2 = np.ones(len(mountain_imgs),)\n",
    "\n",
    "y = np.append(y1,y2)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data:\n",
    "x_in = x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(702, 256, 256, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(x_in))\n",
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (702, 256, 256, 3) y: (702,)\n"
     ]
    }
   ],
   "source": [
    "print('x:',x_in.shape,'y:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "# Import resnet model without the fully connected top layers?\n",
    "resnet = ResNet50(weights='imagenet',include_top=False,input_shape=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='selu')(x)\n",
    "predictions = Dense(1,activation='sigmoid')(x)\n",
    "model = Model(resnet.input,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "dir(K.backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Apps\\Anaconda3\\envs\\U4-S3-DNN\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_in,y,\n",
    "          epochs=1,\n",
    "          steps_per_epoch=5,\n",
    "          batch_size=64\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN Model\n",
    "\n",
    "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 215, 215, 32)      9632      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 39, 39, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 64)          102464    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                331840    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 495,265\n",
      "Trainable params: 495,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.2667 - accuracy: 0.9073 - val_loss: 0.1186 - val_accuracy: 0.9858\n",
      "Epoch 2/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.2046 - accuracy: 0.9073 - val_loss: 0.3342 - val_accuracy: 0.8511\n",
      "Epoch 3/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1778 - accuracy: 0.9287 - val_loss: 0.2746 - val_accuracy: 0.8723\n",
      "Epoch 4/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1681 - accuracy: 0.9323 - val_loss: 0.8487 - val_accuracy: 0.5957\n",
      "Epoch 5/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1606 - accuracy: 0.9394 - val_loss: 0.3903 - val_accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f97388777f0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN Model with Image Manipulations\n",
    "## *This a stretch goal, and it's relatively difficult*\n",
    "\n",
    "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Check out these resources to help you get started: \n",
    "\n",
    "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
    "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Code for Image Manipulation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "# Resources and Stretch Goals\n",
    "\n",
    "Stretch goals\n",
    "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
    "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
    "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
    "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
    "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
    "\n",
    "Resources\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
    "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
    "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
    "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
    "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3-DNN (Python3)",
   "language": "python",
   "name": "u4-s3-dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
